data = read.table("prostate.data", header = TRUE)
data = read.table("prostate.data", header = TRUE)
head(data)
# Régression linéaire multiple
modele <- lm(lpsa ~ ., data = data)
# Résumé du modèle pour obtenir les coefficients et les p-values
summary_modele <- summary(modele)
# Afficher le résumé du modèle
print(summary_modele)
# Extraire les coefficients et les p-values
coefficients <- summary_modele$coefficients
p_values <- coefficients[,4]
# Vérifier quels coefficients sont significativement non nuls (p < 0.05)
significant_coeffs <- coefficients[p_values < 0.05, ]
print("Coefficients significativement non nuls :")
print(significant_coeffs)
# Tester la significativité globale de la régression (test F)
p_value_F <- summary_modele$fstatistic[3]
print(paste("P-value du test F pour la significativité globale :", p_value_F))
if (p_value_F < 0.05) {
print("La régression est globalement significative.")
} else {
print("La régression n'est pas globalement significative.")
}
```{r}
# Replace 'your_file.csv' with the path to your regression data file
library(ggcorrplot)
library(ggpl)
install.packages("ggpl")
install.packages("ggpl2")
install.packages("ggpl", repos="http://cran.us.r-project.org")
# Replace 'your_file.csv' with the path to your regression data file
library(ggcorrplot)
library(ggpl)
# Replace 'your_file.csv' with the path to your regression data file
library(ggcorrplot)
file_path = './a24_reg_app.txt'
X.clas = read.delim(file = file_path, header = TRUE, sep = " ")
# Display the first few rows of the dataframe
head(X.clas)
#plot correlation matrix
corr <- round(cor(X.clas), 1)
ggcorrplot(corr, hc.order = TRUE, type = "lower", lab = TRUE)
library(leaps)
install.packages("rcpp")
install.packages("Rcpp")
install.packages("leaps")
library(leaps)
install.packages("leaps")
install.packages("leaps")
library(leaps)
# reg.fit <- regsubsets(y ~ ., data = X.clas, method = "forward")
library(leaps)
reg.fit <- regsubsets(y ~ ., data = X.clas, method = "forward")
library(leaps)
reg.fit <- regsubsets(y ~ ., data = X.clas, method = "forward")
library(leaps)
reg.fit <- regsubsets(y ~ ., data = X.clas, method = "forward")
plot(reg.fit,scale="r2")
library(leaps)
reg.fit <- regsubsets(y ~ ., data = X.clas, method = "forward")
plot(reg.fit,scale="r2")
summary(reg.git)
library(leaps)
reg.fit <- regsubsets(y ~ ., data = X.clas, method = "forward")
plot(reg.fit,scale="r2")
summary(reg.fit)
install.packages("glmnet")
install.packages("RcppEigen")
install.packages("RcppEigen")
install.packages("glmnet")
install.packages("RcppEigen")
install.packages("Seurat")
install.packages("SeuratObject")
setwd("~/Desktop/GI05/SY19/UTC_SY19")
data <- read.table("a24_reg_app.txt", header = TRUE, sep = " ")
data
data <- read.table("a24_reg_app.txt", header = TRUE, sep = " ")
data
library(caret)
install.packages("caret")
data <- read.table("a24_reg_app.txt", header = TRUE, sep = " ")
data
library(caret)
library(ggplot2)
library(lattice)
set.seed(123)
data <- read.table("a24_reg_app.txt", header = TRUE, sep = " ")
cross_validate_linear_regression <- function(data, target) {
cv_errors <- numeric()
for (k in 3:20) {
train_control <- trainControl(method = "cv", number = k)
model <- train(as.formula(paste(target, "~ .")),
data = data,
method = "lm",
trControl = train_control)
cv_errors[k - 2] <- mean(model$resample$RMSE)
}
best_k <- which.min(cv_errors) + 2
final_train_control <- trainControl(method = "cv", number = best_k)
final_model <- train(as.formula(paste(target, "~ .")),
data = data,
method = "lm",
trControl = final_train_control)
return(list(model = final_model, best_k = best_k))
}
result <- cross_validate_linear_regression(data, "y")
print(result$best_k)
print(result$model)
install.packages("glmnet")
install.packages("RcppEigen")
install.packages("RcppEigen")
install.packages("glmnet")
data <- read.table("a24_reg_app.txt", header = TRUE, sep = " ")
data
library(caret)
library(glmnet)     # For ridge regression models
library(ggplot2)
library(lattice)
set.seed(123)
data <- read.table("a24_reg_app.txt", header = TRUE, sep = " ")
cross_validate_ridge_regression <- function(data, target) {
cv_errors <- numeric()
# Step 1: Find the best number of folds (k) by cross-validation
for (k in 3:10) {  # Limiting k to avoid small sample sizes per fold
train_control <- trainControl(method = "cv",
number = k,
na.action = na.omit)  # Handle missing values in folds
# Train the ridge regression model with the current number of folds
model <- train(as.formula(paste(target, "~ .")),
data = data,
method = "glmnet",
trControl = train_control,
tuneGrid = expand.grid(alpha = 0,    # Alpha = 0 for ridge regression
lambda = seq(0.001, 0.1, length = 10)))  # Lambda values to tune
# Check for any NA values in the RMSE to avoid missing values in results
if (!any(is.na(model$resample$RMSE))) {
cv_errors[k - 2] <- mean(model$resample$RMSE)
} else {
cv_errors[k - 2] <- NA  # Skip storing if NA is found
}
}
# Step 2: Filter out any NA values from cv_errors to avoid issues
cv_errors <- na.omit(cv_errors)
best_k <- which.min(cv_errors) + 2
# Step 3: Train the final ridge regression model using the best number of folds
final_train_control <- trainControl(method = "cv",
number = best_k,
na.action = na.omit)  # Handle missing values in folds
final_model <- train(as.formula(paste(target, "~ .")),
data = data,
method = "glmnet",
trControl = final_train_control,
tuneGrid = expand.grid(alpha = 0,    # Alpha = 0 for ridge regression
lambda = seq(0.001, 0.1, length = 10)))  # Lambda values to tune
# Step 4: Retrieve the R-squared value for the final model
final_r_squared <- max(final_model$results$Rsquared, na.rm = TRUE)  # Handle any NA values in R-squared
return(list(model = final_model, best_k = best_k, R_squared = final_r_squared))
}
# Run the function and print results
result <- cross_validate_ridge_regression(data, "y")
library(caret)
library(glmnet)     # For ridge regression models
library(ggplot2)
library(lattice)
set.seed(123)
data <- read.table("a24_reg_app.txt", header = TRUE, sep = " ")
cross_validate_ridge_regression <- function(data, target) {
cv_errors <- numeric()
# Step 1: Find the best number of folds (k) by cross-validation
for (k in 3:10) {  # Limiting k to avoid small sample sizes per fold
train_control <- trainControl(method = "cv",
number = k)  # Handle missing values in folds
# Train the ridge regression model with the current number of folds
model <- train(as.formula(paste(target, "~ .")),
data = data,
method = "glmnet",
trControl = train_control,
tuneGrid = expand.grid(alpha = 0,    # Alpha = 0 for ridge regression
lambda = seq(0.001, 0.1, length = 10)))  # Lambda values to tune
# Check for any NA values in the RMSE to avoid missing values in results
if (!any(is.na(model$resample$RMSE))) {
cv_errors[k - 2] <- mean(model$resample$RMSE)
} else {
cv_errors[k - 2] <- NA  # Skip storing if NA is found
}
}
# Step 2: Filter out any NA values from cv_errors to avoid issues
cv_errors <- na.omit(cv_errors)
best_k <- which.min(cv_errors) + 2
# Step 3: Train the final ridge regression model using the best number of folds
final_train_control <- trainControl(method = "cv",
number = best_k)  # Handle missing values in folds
final_model <- train(as.formula(paste(target, "~ .")),
data = data,
method = "glmnet",
trControl = final_train_control,
tuneGrid = expand.grid(alpha = 0,    # Alpha = 0 for ridge regression
lambda = seq(0.001, 0.1, length = 10)))  # Lambda values to tune
# Step 4: Retrieve the R-squared value for the final model
final_r_squared <- max(final_model$results$Rsquared, na.rm = TRUE)  # Handle any NA values in R-squared
return(list(model = final_model, best_k = best_k, R_squared = final_r_squared))
}
# Run the function and print results
result <- cross_validate_ridge_regression(data, "y")
print(result$best_k)         # Best number of folds for CV
print(result$R_squared)      # Final model R-squared
print(result$model)          # Final model details
