{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 14\n",
      "Linear Regression \n",
      "\n",
      "500 samples\n",
      "100 predictors\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (14 fold) \n",
      "Summary of sample sizes: 464, 464, 465, 465, 464, 464, ... \n",
      "Resampling results:\n",
      "\n",
      "  RMSE     Rsquared   MAE     \n",
      "  11.9659  0.9568383  9.521029\n",
      "\n",
      "Tuning parameter 'intercept' was held constant at a value of TRUE\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(ggplot2)\n",
    "library(lattice)\n",
    "set.seed(123)\n",
    "data <- read.table(\"a24_reg_app.txt\", header = TRUE, sep = \" \")\n",
    "\n",
    "cross_validate_linear_regression <- function(data, target) {\n",
    "    cv_errors <- numeric()\n",
    "    for (k in 3:20) {\n",
    "        train_control <- trainControl(method = \"cv\", number = k)\n",
    "        model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                                     data = data,\n",
    "                                     method = \"lm\",\n",
    "                                     trControl = train_control)\n",
    "        cv_errors[k - 2] <- mean(model$resample$RMSE)\n",
    "    }\n",
    "    best_k <- which.min(cv_errors) + 2\n",
    "    final_train_control <- trainControl(method = \"cv\", number = best_k)\n",
    "    final_model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                                             data = data,\n",
    "                                             method = \"lm\",\n",
    "                                             trControl = final_train_control)\n",
    "    \n",
    "    return(list(model = final_model, best_k = best_k))\n",
    "}\n",
    "result <- cross_validate_linear_regression(data, \"y\")\n",
    "print(result$best_k)\n",
    "print(result$model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 16\n",
      "[1] 25\n",
      "$model\n",
      "k-Nearest Neighbors \n",
      "\n",
      "500 samples\n",
      "100 predictors\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (16 fold) \n",
      "Summary of sample sizes: 471, 468, 470, 468, 468, 470, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  k   RMSE      Rsquared   MAE     \n",
      "   3  54.57824  0.1611553  43.23926\n",
      "   4  52.35680  0.1964717  41.27100\n",
      "   5  51.68935  0.2066666  40.79558\n",
      "   6  50.68158  0.2415582  40.11442\n",
      "   7  50.04156  0.2644493  39.26169\n",
      "   8  49.46606  0.2913562  39.15450\n",
      "   9  49.57175  0.2952360  39.12756\n",
      "  10  49.54900  0.3031670  39.19183\n",
      "  11  49.27341  0.3197940  39.21594\n",
      "  12  49.20791  0.3303331  39.29317\n",
      "  13  48.81528  0.3612977  38.99966\n",
      "  14  48.45992  0.3915724  38.49296\n",
      "  15  48.37063  0.3985982  38.36468\n",
      "  16  48.39756  0.4088595  38.28730\n",
      "  17  48.38141  0.4153013  38.31843\n",
      "  18  48.19710  0.4372807  38.19778\n",
      "  19  48.52003  0.4282907  38.50323\n",
      "  20  48.34222  0.4422701  38.43515\n",
      "  21  48.43653  0.4452800  38.57103\n",
      "  22  48.43711  0.4549084  38.49626\n",
      "  23  48.40532  0.4653965  38.35540\n",
      "  24  48.26753  0.4883288  38.22634\n",
      "  25  48.19043  0.4988087  38.13347\n",
      "\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final value used for the model was k = 25.\n",
      "\n",
      "$best_k\n",
      "[1] 16\n",
      "\n",
      "$best_k_prime\n",
      "[1] 25\n",
      "\n",
      "$R_squared\n",
      "[1] 0.4988087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(ggplot2)\n",
    "library(lattice)\n",
    "set.seed(123)\n",
    "\n",
    "data <- read.table(\"a24_reg_app.txt\", header = TRUE, sep = \" \")\n",
    "\n",
    "cross_validate_knn_regression <- function(data, target) {\n",
    "    # Step 1: Find the best number of folds k for cross-validation\n",
    "    cv_errors <- numeric()\n",
    "    for (k in 3:20) {\n",
    "        train_control <- trainControl(method = \"cv\", number = k)\n",
    "        \n",
    "        # Tune k' (number of neighbors) within a range using cross-validation for each fold number\n",
    "        model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                       data = data,\n",
    "                       method = \"knn\",\n",
    "                       tuneGrid = data.frame(k = 3:25),  # Try neighbors from 1 to 20\n",
    "                       trControl = train_control)\n",
    "        \n",
    "        # Store the lowest RMSE for each k\n",
    "        cv_errors[k - 2] <- min(model$results$RMSE)\n",
    "    }\n",
    "    \n",
    "    # Step 2: Select the best number of folds k\n",
    "    best_k <- which.min(cv_errors) + 2\n",
    "    \n",
    "    # Step 3: Using best_k, find the best number of neighbors k' for the final model\n",
    "    final_train_control <- trainControl(method = \"cv\", number = best_k)\n",
    "    final_model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                         data = data,\n",
    "                         method = \"knn\",\n",
    "                         tuneGrid = data.frame(k = 3:25),  # Tune k' from 1 to 20\n",
    "                         trControl = final_train_control)\n",
    "    \n",
    "    # Get the best number of neighbors\n",
    "    best_k_prime <- final_model$bestTune$k\n",
    "    \n",
    "    # Step 4: Calculate R-squared for the final model\n",
    "    final_r_squared <- max(final_model$results$Rsquared)\n",
    "    \n",
    "    return(list(model = final_model, best_k = best_k, best_k_prime = best_k_prime, R_squared = final_r_squared))\n",
    "}\n",
    "\n",
    "# Run the function and print results\n",
    "result <- cross_validate_knn_regression(data, \"y\")\n",
    "print(result$best_k)         # Best number of folds for CV\n",
    "print(result$best_k_prime)    # Best number of neighbors\n",
    "print(result)       # Final model R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "“There were missing values in resampled performance measures.”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 8\n",
      "[1] 0.1643124\n",
      "CART \n",
      "\n",
      "500 samples\n",
      "100 predictors\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (8 fold) \n",
      "Summary of sample sizes: 436, 437, 439, 440, 438, 436, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  cp          RMSE      Rsquared    MAE     \n",
      "  0.05532061  53.50864  0.16431237  42.50041\n",
      "  0.05978122  53.90575  0.14821406  43.27662\n",
      "  0.14324339  57.58948  0.05434224  46.31702\n",
      "\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final value used for the model was cp = 0.05532061.\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(rpart)      # For decision tree models\n",
    "library(ggplot2)\n",
    "library(lattice)\n",
    "set.seed(123)\n",
    "\n",
    "data <- read.table(\"a24_reg_app.txt\", header = TRUE, sep = \" \")\n",
    "\n",
    "cross_validate_decision_tree <- function(data, target) {\n",
    "    cv_errors <- numeric()\n",
    "    \n",
    "    # Step 1: Find the best number of folds (k) by cross-validation\n",
    "    for (k in 3:10) {  # Reduced upper bound to avoid excessive folding on small datasets\n",
    "        train_control <- trainControl(method = \"cv\", \n",
    "                                      number = k)\n",
    "        \n",
    "        # Train the decision tree model with the current number of folds\n",
    "        model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                       data = data,\n",
    "                       method = \"rpart\",\n",
    "                       trControl = train_control)\n",
    "        \n",
    "        # Check for any NA values in the RMSE to avoid missing values in results\n",
    "        if (!any(is.na(model$resample$RMSE))) {\n",
    "            cv_errors[k - 2] <- mean(model$resample$RMSE)\n",
    "        } else {\n",
    "            cv_errors[k - 2] <- NA  # Skip storing if NA is found\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 2: Filter out any NA values from cv_errors to avoid issues\n",
    "    cv_errors <- na.omit(cv_errors)\n",
    "    best_k <- which.min(cv_errors) + 2\n",
    "    \n",
    "    # Step 3: Train the final decision tree model using the best number of folds\n",
    "    final_train_control <- trainControl(method = \"cv\", \n",
    "                                        number = best_k)\n",
    "    \n",
    "    final_model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                         data = data,\n",
    "                         method = \"rpart\",\n",
    "                         trControl = final_train_control)\n",
    "    \n",
    "    # Step 4: Retrieve the R-squared value for the final model\n",
    "    final_r_squared <- max(final_model$results$Rsquared, na.rm = TRUE)  # Handle any NA values in R-squared\n",
    "    \n",
    "    return(list(model = final_model, best_k = best_k, R_squared = final_r_squared))\n",
    "}\n",
    "\n",
    "# Run the function and print results\n",
    "result <- cross_validate_decision_tree(data, \"y\")\n",
    "print(result$best_k)         # Best number of folds for CV\n",
    "print(result$R_squared)      # Final model R-squared\n",
    "print(result$model)          # Final model details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 7\n",
      "[1] 0.6643114\n",
      "Random Forest \n",
      "\n",
      "500 samples\n",
      "100 predictors\n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (7 fold) \n",
      "Summary of sample sizes: 428, 429, 429, 429, 428, 428, ... \n",
      "Resampling results across tuning parameters:\n",
      "\n",
      "  mtry  RMSE      Rsquared   MAE     \n",
      "  1     53.50857  0.5538439  42.56899\n",
      "  2     52.27357  0.6020620  41.54268\n",
      "  3     51.13092  0.6643114  40.57656\n",
      "\n",
      "RMSE was used to select the optimal model using the smallest value.\n",
      "The final value used for the model was mtry = 3.\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(randomForest) # For random forest models\n",
    "library(ggplot2)\n",
    "library(lattice)\n",
    "set.seed(123)\n",
    "\n",
    "data <- read.table(\"a24_reg_app.txt\", header = TRUE, sep = \" \")\n",
    "\n",
    "cross_validate_random_forest <- function(data, target) {\n",
    "    cv_errors <- numeric()\n",
    "    \n",
    "    # Step 1: Find the best number of folds (k) by cross-validation\n",
    "    for (k in 3:10) {  # Limiting k to avoid small sample sizes per fold\n",
    "        train_control <- trainControl(method = \"cv\",\n",
    "                                      number = k) \n",
    "        grid_rf <- expand.grid(mtry = c(1:3))  # Hyperparameter grid for mtry\n",
    "        # Train the random forest model with the current number of folds\n",
    "        model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                       data = data,\n",
    "                       method = \"rf\",\n",
    "                       metric = \"Rsquared\",\n",
    "                       trControl = train_control,\n",
    "                       tuneGrid = grid_rf)  # Adjust tuneLength as needed for hyperparameter tuning\n",
    "        \n",
    "        # Check for any NA values in the RMSE to avoid missing values in results\n",
    "        if (!any(is.na(model$resample$RMSE))) {\n",
    "            cv_errors[k - 2] <- mean(model$resample$RMSE)\n",
    "        } else {\n",
    "            cv_errors[k - 2] <- NA  # Skip storing if NA is found\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 2: Filter out any NA values from cv_errors to avoid issues\n",
    "    cv_errors <- na.omit(cv_errors)\n",
    "    best_k <- which.min(cv_errors) + 2\n",
    "    \n",
    "    # Step 3: Train the final random forest model using the best number of folds\n",
    "    final_train_control <- trainControl(method = \"cv\", \n",
    "                                        number = best_k)\n",
    "    grid_rf <- expand.grid(mtry = c(1:3))\n",
    "    final_model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                         data = data,\n",
    "                         method = \"rf\",\n",
    "                         trControl = final_train_control,\n",
    "                         tuneGrid = grid_rf)  # Adjust tuneLength as needed for hyperparameter tuning\n",
    "    \n",
    "    # Step 4: Retrieve the R-squared value for the final model\n",
    "    final_r_squared <- max(final_model$results$Rsquared, na.rm = TRUE)  # Handle any NA values in R-squared\n",
    "    \n",
    "    return(list(model = final_model, best_k = best_k, R_squared = final_r_squared))\n",
    "}\n",
    "\n",
    "# Run the function and print results\n",
    "result <- cross_validate_random_forest(data, \"y\")\n",
    "print(result$best_k)         # Best number of folds for CV\n",
    "print(result$R_squared)      # Final model R-squared\n",
    "print(result$model)          # Final model details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop ICI, cette partie n'est pas encore re travailler et n'est pas encore fonctionnelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in df - 1L: non-numeric argument to binary operator\n",
     "output_type": "error",
     "traceback": [
      "Error in df - 1L: non-numeric argument to binary operator\nTraceback:\n",
      "1. cross_validate_cubic_spline(data, \"y\")",
      "2. train(as.formula(paste(target, \"~ ns(\", predictor, \", knots)\")), \n .     data = data, method = \"lm\", trControl = train_control, tuneGrid = data.frame(knots = 1:10))   # at line 10-14 of file <text>",
      "3. train.formula(as.formula(paste(target, \"~ ns(\", predictor, \", knots)\")), \n .     data = data, method = \"lm\", trControl = train_control, tuneGrid = data.frame(knots = 1:10))",
      "4. eval.parent(m)",
      "5. eval(expr, p)",
      "6. eval(expr, p)",
      "7. stats::model.frame(form = as.formula(paste(target, \"~ ns(\", predictor, \n .     \", knots)\")), data = data, na.action = na.fail)",
      "8. model.frame.default(form = as.formula(paste(target, \"~ ns(\", \n .     predictor, \", knots)\")), data = data, na.action = na.fail)",
      "9. eval(predvars, data, env)",
      "10. eval(predvars, data, env)",
      "11. ns(X1, knots)"
     ]
    }
   ],
   "source": [
    "cross_validate_cubic_spline <- function(data, target) {\n",
    "    predictor <- names(data)[names(data) != target][1]  # Assuming the first predictor variable\n",
    "    \n",
    "    # Step 1: Find the best number of folds k for cross-validation\n",
    "    cv_errors <- numeric()\n",
    "    for (k in 3:20) {\n",
    "        train_control <- trainControl(method = \"cv\", number = k)\n",
    "        \n",
    "        # Tune the number of knots by trying different numbers of knots (1 to 10 in this example)\n",
    "        model <- train(as.formula(paste(target, \"~ ns(\", predictor, \", knots)\")),\n",
    "                       data = data,\n",
    "                       method = \"lm\",\n",
    "                       trControl = train_control,\n",
    "                       tuneGrid = data.frame(knots = 1:10))  # Adjust number of knots\n",
    "        \n",
    "        # Store the lowest RMSE for each k\n",
    "        cv_errors[k - 2] <- min(model$results$RMSE)\n",
    "    }\n",
    "    \n",
    "    # Step 2: Select the best number of folds k\n",
    "    best_k <- which.min(cv_errors) + 2\n",
    "    \n",
    "    # Step 3: Using best_k, find the optimal number of knots\n",
    "    final_train_control <- trainControl(method = \"cv\", number = best_k)\n",
    "    final_model <- train(as.formula(paste(target, \"~ ns(\", predictor, \", knots)\")),\n",
    "                         data = data,\n",
    "                         method = \"lm\",\n",
    "                         trControl = final_train_control,\n",
    "                         tuneGrid = data.frame(knots = 1:10))  # Adjust number of knots\n",
    "    \n",
    "    # Get the best number of knots\n",
    "    best_knots <- final_model$bestTune$knots\n",
    "    \n",
    "    # Step 4: Calculate R-squared for the final model\n",
    "    final_r_squared <- max(final_model$results$Rsquared)\n",
    "    \n",
    "    return(list(model = final_model, best_k = best_k, best_knots = best_knots, R_squared = final_r_squared))\n",
    "}\n",
    "\n",
    "# Run the function and print results\n",
    "result <- cross_validate_cubic_spline(data, \"y\")\n",
    "print(result$best_k)         # Best number of folds for CV\n",
    "print(result$best_knots)     # Optimal number of knots\n",
    "print(result$R_squared)      # Final model R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(glmnet): there is no package called ‘glmnet’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(glmnet): there is no package called ‘glmnet’\nTraceback:\n",
      "1. library(glmnet)"
     ]
    }
   ],
   "source": [
    "\n",
    "library(caret)\n",
    "library(MASS)       # For QDA and LDA\n",
    "library(splines)    # For Splines\n",
    "library(class)      # For KNN\n",
    "library(glmnet)     # For Ridge, Lasso, and Elastic Net Regression\n",
    "library(rpart)      # For Decision Trees\n",
    "library(Matrix)\n",
    "set.seed(123)\n",
    "data <- read.table(\"a24_reg_app.txt\", header = TRUE, sep = \" \")\n",
    "\n",
    "cross_validate_models <- function(data, target, model_type) {\n",
    "    cv_errors <- numeric()\n",
    "    best_k <- 0\n",
    "    final_model <- NULL\n",
    "\n",
    "    for (k in 3:20) {\n",
    "        train_control <- trainControl(method = \"cv\", number = k)\n",
    "\n",
    "        # Select method based on model_type\n",
    "        if (model_type == \"linear_regression\") {\n",
    "            model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                           data = data,\n",
    "                           method = \"lm\",\n",
    "                           trControl = train_control)\n",
    "\n",
    "        } else if (model_type == \"ridge_regression\") {\n",
    "            formula <- as.formula(paste(target, \"~ .\"))\n",
    "            model <- train(formula,\n",
    "                           data = data,\n",
    "                           method = \"glmnet\",\n",
    "                           trControl = train_control,\n",
    "                           tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 1, length = 100)))\n",
    "\n",
    "        } else if (model_type == \"lasso_regression\") {\n",
    "            formula <- as.formula(paste(target, \"~ .\"))\n",
    "            model <- train(formula,\n",
    "                           data = data,\n",
    "                           method = \"glmnet\",\n",
    "                           trControl = train_control,\n",
    "                           tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 1, length = 100)))\n",
    "\n",
    "        } else if (model_type == \"elastic_net_regression\") {\n",
    "            formula <- as.formula(paste(target, \"~ .\"))\n",
    "            model <- train(formula,\n",
    "                           data = data,\n",
    "                           method = \"glmnet\",\n",
    "                           trControl = train_control,\n",
    "                           tuneGrid = expand.grid(alpha = seq(0, 1, length = 10), lambda = seq(0, 1, length = 100)))\n",
    "\n",
    "        } else if (model_type == \"splines\") {\n",
    "            model <- train(as.formula(paste(target, \"~ bs(. , degree=3)\")),\n",
    "                           data = data,\n",
    "                           method = \"lm\",\n",
    "                           trControl = train_control)\n",
    "\n",
    "        } else if (model_type == \"knn\") {\n",
    "            model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                           data = data,\n",
    "                           method = \"knn\",\n",
    "                           trControl = train_control,\n",
    "                           tuneGrid = expand.grid(k = k))  # k here is used in KNN\n",
    "\n",
    "        } else {\n",
    "            stop(\"Invalid model type. Choose from 'linear_regression', 'polynomial_regression', 'ridge_regression', 'lasso_regression', 'elastic_net_regression', 'splines', or 'knn'.\")\n",
    "        }\n",
    "\n",
    "        # Calculate cross-validation error based on model type\n",
    "        if (model_type %in% c(\"linear_regression\", \"polynomial_regression\", \"ridge_regression\", \"lasso_regression\", \"elastic_net_regression\", \"splines\")) {\n",
    "            cv_errors[k - 2] <- mean(model$resample$RMSE)\n",
    "        } else if (model_type == \"knn\") {\n",
    "            cv_errors[k - 2] <- 1 - mean(model$resample$Accuracy)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Find the value of k that minimizes the cross-validation error\n",
    "    best_k <- which.min(cv_errors) + 2\n",
    "\n",
    "    # Retrain the model with the optimal k\n",
    "    final_train_control <- trainControl(method = \"cv\", number = best_k)\n",
    "\n",
    "    if (model_type == \"linear_regression\") {\n",
    "        final_model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                             data = data,\n",
    "                             method = \"lm\",\n",
    "                             trControl = final_train_control)\n",
    "\n",
    "    } else if (model_type == \"ridge_regression\") {\n",
    "        formula <- as.formula(paste(target, \"~ .\"))\n",
    "        final_model <- train(formula,\n",
    "                             data = data,\n",
    "                             method = \"glmnet\",\n",
    "                             trControl = final_train_control,\n",
    "                             tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 1, length = 100)))\n",
    "\n",
    "    } else if (model_type == \"lasso_regression\") {\n",
    "        formula <- as.formula(paste(target, \"~ .\"))\n",
    "        final_model <- train(formula,\n",
    "                             data = data,\n",
    "                             method = \"glmnet\",\n",
    "                             trControl = final_train_control,\n",
    "                             tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 1, length = 100)))\n",
    "\n",
    "    } else if (model_type == \"elastic_net_regression\") {\n",
    "        formula <- as.formula(paste(target, \"~ .\"))\n",
    "        final_model <- train(formula,\n",
    "                             data = data,\n",
    "                             method = \"glmnet\",\n",
    "                             trControl = final_train_control,\n",
    "                             tuneGrid = expand.grid(alpha = seq(0, 1, length = 10), lambda = seq(0, 1, length = 100)))\n",
    "\n",
    "    } else if (model_type == \"splines\") {\n",
    "        final_model <- train(as.formula(paste(target, \"~ bs(. , degree=3)\")),\n",
    "                             data = data,\n",
    "                             method = \"lm\",\n",
    "                             trControl = final_train_control)\n",
    "\n",
    "    } else if (model_type == \"knn\") {\n",
    "        final_model <- train(as.formula(paste(target, \"~ .\")),\n",
    "                             data = data,\n",
    "                             method = \"knn\",\n",
    "                             trControl = final_train_control,\n",
    "                             tuneGrid = expand.grid(k = best_k))\n",
    "    }\n",
    "\n",
    "    # Return the final model and the best k\n",
    "    return(list(model = final_model, best_k = best_k))\n",
    "}\n",
    "\n",
    "# Example calls:\n",
    "# For Linear Regression\n",
    "result_lr <- cross_validate_models(data, \"y\", \"linear_regression\")\n",
    "print(result_lr$best_k)\n",
    "print(result_lr$model)\n",
    "\n",
    "# For Ridge Regression\n",
    "result_rr <- cross_validate_models(data, \"y\", \"ridge_regression\")\n",
    "print(result_rr$best_k)\n",
    "print(result_rr$model)\n",
    "\n",
    "# For Lasso Regression\n",
    "result_lar <- cross_validate_models(data, \"y\", \"lasso_regression\")\n",
    "print(result_lar$best_k)\n",
    "print(result_lar$model)\n",
    "\n",
    "# For Elastic Net Regression\n",
    "result_enr <- cross_validate_models(data, \"y\", \"elastic_net_regression\")\n",
    "print(result_enr$best_k)\n",
    "print(result_enr$model)\n",
    "\n",
    "# For Splines\n",
    "result_splines <- cross_validate_models(data, \"y\", \"splines\")\n",
    "print(result_splines$best_k)\n",
    "print(result_splines$model)\n",
    "\n",
    "# For KNN\n",
    "result_knn <- cross_validate_models(data, \"y\", \"knn\")\n",
    "print(result_knn$best_k)\n",
    "print(result_knn$model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
