{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and installing required packages...\n",
      "Package 'caret' is already installed and loaded\n",
      "Package 'MASS' is already installed and loaded\n",
      "Package 'randomForest' is already installed and loaded\n",
      "Package 'e1071' is already installed and loaded\n",
      "Package 'ggplot2' is already installed and loaded\n",
      "Package 'dplyr' is already installed and loaded\n",
      "Package 'corrplot' is already installed and loaded\n",
      "Package 'nnet' is already installed and loaded\n",
      "\n",
      "All required packages are installed and loaded!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "required_packages <- c(\n",
    "\"caret\",\n",
    "\"MASS\",\n",
    "\"randomForest\",\n",
    "\"e1071\",\n",
    "\"ggplot2\",\n",
    "\"dplyr\",\n",
    "\"corrplot\",\n",
    "\"nnet\"\n",
    ")\n",
    "\n",
    "# Function to install and load packages\n",
    "install_and_load_packages <- function(packages) {\n",
    "  cat(\"Checking and installing required packages...\\n\")\n",
    "  \n",
    "  for (package in packages) {\n",
    "    if (!require(package, character.only = TRUE, quietly = TRUE)) {\n",
    "      cat(sprintf(\"Installing package: %s\\n\", package))\n",
    "      install.packages(package, dependencies = TRUE)\n",
    "      if (!require(package, character.only = TRUE, quietly = TRUE)) {\n",
    "        stop(sprintf(\"Package '%s' installation failed\", package))\n",
    "      }\n",
    "    } else {\n",
    "      cat(sprintf(\"Package '%s' is already installed and loaded\\n\", package))\n",
    "    }\n",
    "  }\n",
    "  cat(\"\\nAll required packages are installed and loaded!\\n\\n\")\n",
    "}\n",
    "\n",
    "# Install and load all required packages\n",
    "install_and_load_packages(required_packages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "library(MASS)\n",
    "library(randomForest)\n",
    "library(e1071)\n",
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "library(corrplot)\n",
    "library(nnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data <- read.table(\"a24_clas_app.txt\", header = TRUE,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows deleted due to outliers: 27.6 %\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error: wrong model type for regression\n",
     "output_type": "error",
     "traceback": [
      "Error: wrong model type for regression\nTraceback:\n",
      "1. train(y ~ ., data = inner_train_data, method = method, trControl = trainControl(method = \"cv\", \n .     number = inner_folds), metric = \"Accuracy\")",
      "2. train.formula(y ~ ., data = inner_train_data, method = method, \n .     trControl = trainControl(method = \"cv\", number = inner_folds), \n .     metric = \"Accuracy\")",
      "3. train(x, y, weights = w, ...)",
      "4. train.default(x, y, weights = w, ...)",
      "5. stop(paste(\"wrong model type for\", tolower(modelType)), call. = FALSE)"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries\n",
    "library(caret)\n",
    "library(dplyr)\n",
    "library(nnet)     # for multinomial logistic regression\n",
    "library(ggplot2)  # for plotting\n",
    "\n",
    "# Make a copy of the original data for processing\n",
    "data_cleaned <- data\n",
    "\n",
    "# Identify excluded variables (0-13 values)\n",
    "excluded_vars <- c(\"X46\", \"X47\", \"X48\", \"X49\", \"X50\") \n",
    "numeric_vars <- names(data_cleaned)[sapply(data_cleaned, is.numeric) & !(names(data_cleaned) %in% excluded_vars) & names(data_cleaned) != \"y\"]\n",
    "\n",
    "# Outlier Removal\n",
    "# ------------------\n",
    "# Calculate skewness and IQR filtering for outliers on selected numeric variables only\n",
    "for (var in numeric_vars) {\n",
    "  Q1 <- quantile(data_cleaned[[var]], 0.25, na.rm = TRUE)\n",
    "  Q3 <- quantile(data_cleaned[[var]], 0.75, na.rm = TRUE)\n",
    "  IQR_val <- Q3 - Q1\n",
    "  \n",
    "  lower_bound <- Q1 - 1.5 * IQR_val\n",
    "  upper_bound <- Q3 + 1.5 * IQR_val\n",
    "  \n",
    "  # Remove rows with outliers in any numeric variable\n",
    "  data_cleaned <- data_cleaned[!(data_cleaned[[var]] < lower_bound | data_cleaned[[var]] > upper_bound), ]\n",
    "}\n",
    "\n",
    "# Calculate the ratio of rows deleted\n",
    "cat(\"Rows deleted due to outliers:\", (1 - nrow(data_cleaned) / nrow(data)) * 100, \"%\\n\")\n",
    "\n",
    "# Scaling\n",
    "# ----------\n",
    "# Apply scaling to numeric variables only\n",
    "preprocess_params <- preProcess(data_cleaned[, numeric_vars], method = c(\"center\", \"scale\"))\n",
    "data_scaled <- data_cleaned\n",
    "data_scaled[, numeric_vars] <- predict(preprocess_params, data_cleaned[, numeric_vars])\n",
    "\n",
    "# Separate datasets with and without excluded variables\n",
    "data_with_excluded <- data_scaled\n",
    "data_without_excluded <- data_scaled %>% select(-all_of(excluded_vars))\n",
    "\n",
    "# Nested Cross-Validation (Classification Models)\n",
    "# ---------------------------------------------------\n",
    "# Define outer and inner folds\n",
    "outer_folds <- 5\n",
    "inner_folds <- 3\n",
    "\n",
    "# Define classification-only methods, removing any that might be regression-only\n",
    "classification_methods <- c(\"multinom\", \"qda\", \"lda\", \"naive_bayes\", \"rpart\", \"rf\")\n",
    "\n",
    "# Function to evaluate classification model\n",
    "evaluate_model <- function(model, test_data, true_labels) {\n",
    "  predictions <- predict(model, newdata = test_data)\n",
    "  confusion <- confusionMatrix(predictions, true_labels)\n",
    "  \n",
    "  return(list(\n",
    "    Accuracy = confusion$overall[\"Accuracy\"],\n",
    "    Sensitivity = confusion$byClass[\"Sensitivity\"],\n",
    "    Specificity = confusion$byClass[\"Specificity\"]\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results <- data.frame(Model = character(), Variable_Set = character(), Fold = integer(),\n",
    "                      Accuracy = numeric(), Sensitivity = numeric(), Specificity = numeric())\n",
    "\n",
    "# Loop through each classification model and both datasets\n",
    "for (method in classification_methods) {\n",
    "  for (variable_set in c(\"with_excluded\", \"without_excluded\")) {\n",
    "    \n",
    "    data_set <- if (variable_set == \"with_excluded\") data_with_excluded else data_without_excluded\n",
    "    \n",
    "    for (outer_fold in 1:outer_folds) {\n",
    "      outer_train_index <- createFolds(data_set$y, k = outer_folds, list = TRUE, returnTrain = TRUE)[[outer_fold]]\n",
    "      outer_train_data <- data_set[outer_train_index, ]\n",
    "      outer_test_data <- data_set[-outer_train_index, ]\n",
    "      \n",
    "      best_inner_model <- NULL\n",
    "      best_inner_accuracy <- 0\n",
    "      \n",
    "      for (inner_fold in 1:inner_folds) {\n",
    "        inner_train_index <- createFolds(outer_train_data$y, k = inner_folds, list = TRUE, returnTrain = TRUE)[[inner_fold]]\n",
    "        inner_train_data <- outer_train_data[inner_train_index, ]\n",
    "        inner_val_data <- outer_train_data[-inner_train_index, ]\n",
    "        \n",
    "        # Train the model\n",
    "        model <- train(y ~ ., data = inner_train_data, method = method, trControl = trainControl(method = \"cv\", number = inner_folds), metric = \"Accuracy\")\n",
    "        \n",
    "        # Get accuracy on validation set\n",
    "        val_pred <- predict(model, newdata = inner_val_data)\n",
    "        val_accuracy <- mean(val_pred == inner_val_data$y)\n",
    "        \n",
    "        # Update best model if necessary\n",
    "        if (val_accuracy > best_inner_accuracy) {\n",
    "          best_inner_model <- model\n",
    "          best_inner_accuracy <- val_accuracy\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      # Evaluate the best inner model on outer test set\n",
    "      outer_metrics <- evaluate_model(best_inner_model, outer_test_data, outer_test_data$y)\n",
    "      \n",
    "      # Store results\n",
    "      results <- rbind(results, data.frame(Model = method, Variable_Set = variable_set, Fold = outer_fold,\n",
    "                                           Accuracy = outer_metrics$Accuracy,\n",
    "                                           Sensitivity = outer_metrics$Sensitivity,\n",
    "                                           Specificity = outer_metrics$Specificity))\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Print and Plot Final Results\n",
    "cat(\"\\nClassification Results:\\n\")\n",
    "print(results)\n",
    "\n",
    "# Plot results\n",
    "ggplot(results, aes(x = Model, y = Accuracy, fill = Variable_Set)) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Model Accuracy across Nested Cross-Validation Folds\",\n",
    "       x = \"Model\", y = \"Accuracy\")\n",
    "\n",
    "ggplot(results, aes(x = Model, y = Sensitivity, fill = Variable_Set)) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Model Sensitivity across Nested Cross-Validation Folds\",\n",
    "       x = \"Model\", y = \"Sensitivity\")\n",
    "\n",
    "ggplot(results, aes(x = Model, y = Specificity, fill = Variable_Set)) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Model Specificity across Nested Cross-Validation Folds\",\n",
    "       x = \"Model\", y = \"Specificity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running analysis without preprocessing...\n",
      "Training logistic model...\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in preprocess(X_train_processed): impossible de trouver la fonction \"preprocess\"\n",
     "output_type": "error",
     "traceback": [
      "Error in preprocess(X_train_processed): impossible de trouver la fonction \"preprocess\"\nTraceback:\n",
      "1. nested_cv(X, y, model_type = model, preprocess_params = list(scale = FALSE, \n .     remove_correlated = FALSE, handle_outliers = FALSE))",
      "2. predict(preprocess(X_train_processed), X_test)   # at line 123 of file <text>"
     ]
    }
   ],
   "source": [
    "# # Load required libraries\n",
    "# library(tidyverse)\n",
    "# library(ggplot2)\n",
    "# library(caret)\n",
    "# library(MASS)\n",
    "# library(rpart)\n",
    "# library(randomForest)\n",
    "# library(corrplot)\n",
    "# library(e1071)\n",
    "# library(pROC)\n",
    "# library(moments)\n",
    "# library(gridExtra)\n",
    "\n",
    "# # ---- 1. Data Loading and Initial Setup ----\n",
    "# data <- read.table(\"a24_clas_app.txt\", header = TRUE,sep = \" \")\n",
    "# X <- data[, 1:50]  # Features\n",
    "# y <- as.factor(data[, 51])  # Target variable\n",
    "\n",
    "# # ---- 2. Feature Analysis Functions ----\n",
    "# analyze_features <- function(X) {\n",
    "#   feature_stats <- data.frame(\n",
    "#     mean = colMeans(X),\n",
    "#     sd = apply(X, 2, sd),\n",
    "#     median = apply(X, 2, median),\n",
    "#     skewness = apply(X, 2, skewness),\n",
    "#     kurtosis = apply(X, 2, kurtosis),\n",
    "#     zeros = colSums(X == 0),\n",
    "#     unique_values = apply(X, 2, function(x) length(unique(x)))\n",
    "#   )\n",
    "#   return(feature_stats)\n",
    "# }\n",
    "\n",
    "# detect_outliers <- function(X, method = \"iqr\", threshold = 1.5) {\n",
    "#   outliers_summary <- list()\n",
    "#   for (col in colnames(X)) {\n",
    "#     x <- X[[col]]\n",
    "#     if (method == \"iqr\") {\n",
    "#       Q1 <- quantile(x, 0.25)\n",
    "#       Q3 <- quantile(x, 0.75)\n",
    "#       IQR <- Q3 - Q1\n",
    "#       lower_bound <- Q1 - threshold * IQR\n",
    "#       upper_bound <- Q3 + threshold * IQR\n",
    "#       outliers_summary[[col]] <- list(\n",
    "#         n_outliers = sum(x < lower_bound | x > upper_bound),\n",
    "#         lower_bound = lower_bound,\n",
    "#         upper_bound = upper_bound\n",
    "#       )\n",
    "#     }\n",
    "#   }\n",
    "#   return(outliers_summary)\n",
    "# }\n",
    "\n",
    "# # ---- 3. Preprocessing Function ----\n",
    "# preprocess_data <- function(X, scale = TRUE, remove_correlated = FALSE, \n",
    "#                           correlation_threshold = 0.8, handle_outliers = TRUE) {\n",
    "#   # Handle outliers if requested\n",
    "#   if (handle_outliers) {\n",
    "#     outliers <- detect_outliers(X)\n",
    "#     for (col in colnames(X)) {\n",
    "#       lower_bound <- outliers[[col]]$lower_bound\n",
    "#       upper_bound <- outliers[[col]]$upper_bound\n",
    "#       X[[col]][X[[col]] < lower_bound] <- lower_bound\n",
    "#       X[[col]][X[[col]] > upper_bound] <- upper_bound\n",
    "#     }\n",
    "#   }\n",
    "  \n",
    "#   # Remove highly correlated features if requested\n",
    "#   if (remove_correlated) {\n",
    "#     correlation_matrix <- cor(X)\n",
    "#     highly_correlated <- findCorrelation(correlation_matrix, \n",
    "#                                        cutoff = correlation_threshold)\n",
    "#     if (length(highly_correlated) > 0) {\n",
    "#       X <- X[, -highly_correlated]\n",
    "#     }\n",
    "#   }\n",
    "  \n",
    "#   # Scale features if requested\n",
    "#   if (scale) {\n",
    "#     X <- scale(X)\n",
    "#   }\n",
    "  \n",
    "#   return(X)\n",
    "# }\n",
    "\n",
    "# # ---- 4. Model Evaluation Functions ----\n",
    "# calculate_metrics <- function(pred, actual) {\n",
    "#   confusion_mat <- confusionMatrix(pred, actual)\n",
    "#   metrics <- list(\n",
    "#     accuracy = confusion_mat$overall[\"Accuracy\"],\n",
    "#     sensitivity = mean(confusion_mat$byClass[, \"Sensitivity\"]),\n",
    "#     specificity = mean(confusion_mat$byClass[, \"Specificity\"])\n",
    "#   )\n",
    "#   return(metrics)\n",
    "# }\n",
    "\n",
    "# # ---- 5. Nested Cross-Validation Function ----\n",
    "# nested_cv <- function(X, y, outer_folds = 5, inner_folds = 5, \n",
    "#                      model_type = c(\"logistic\", \"lda\", \"qda\", \"naive_bayes\", \"tree\", \"rf\"),\n",
    "#                      preprocess_params = list(scale = TRUE, \n",
    "#                                            remove_correlated = FALSE,\n",
    "#                                            handle_outliers = TRUE)) {\n",
    "#   outer_cv <- createFolds(y, k = outer_folds, list = TRUE)\n",
    "#   results <- list()\n",
    "#   predictions <- numeric(length(y))\n",
    "  \n",
    "#   for (fold in 1:outer_folds) {\n",
    "#     # Split data\n",
    "#     train_indices <- unlist(outer_cv[-fold])\n",
    "#     test_indices <- outer_cv[[fold]]\n",
    "    \n",
    "#     X_train <- X[train_indices, ]\n",
    "#     y_train <- y[train_indices]\n",
    "#     X_test <- X[test_indices, ]\n",
    "#     y_test <- y[test_indices]\n",
    "    \n",
    "#     # Preprocess data\n",
    "#     X_train_processed <- preprocess_data(X_train, \n",
    "#                                        scale = preprocess_params$scale,\n",
    "#                                        remove_correlated = preprocess_params$remove_correlated,\n",
    "#                                        handle_outliers = preprocess_params$handle_outliers)\n",
    "    \n",
    "#     # Apply same preprocessing to test data\n",
    "#     X_test_processed <- predict(preprocess(X_train_processed), X_test)\n",
    "    \n",
    "#     # Train model based on type\n",
    "#     if (model_type == \"logistic\") {\n",
    "#       model <- multinom(y_train ~ ., data = as.data.frame(X_train_processed))\n",
    "#       pred <- predict(model, newdata = as.data.frame(X_test_processed), type = \"class\")\n",
    "#     } else if (model_type == \"lda\") {\n",
    "#       model <- lda(X_train_processed, y_train)\n",
    "#       pred <- predict(model, X_test_processed)$class\n",
    "#     } else if (model_type == \"qda\") {\n",
    "#       model <- qda(X_train_processed, y_train)\n",
    "#       pred <- predict(model, X_test_processed)$class\n",
    "#     } else if (model_type == \"naive_bayes\") {\n",
    "#       model <- naiveBayes(X_train_processed, y_train)\n",
    "#       pred <- predict(model, X_test_processed)\n",
    "#     } else if (model_type == \"tree\") {\n",
    "#       model <- rpart(y_train ~ ., data = as.data.frame(X_train_processed))\n",
    "#       pred <- predict(model, as.data.frame(X_test_processed), type = \"class\")\n",
    "#     } else if (model_type == \"rf\") {\n",
    "#       model <- randomForest(X_train_processed, y_train)\n",
    "#       pred <- predict(model, X_test_processed)\n",
    "#     }\n",
    "    \n",
    "#     # Store predictions and calculate metrics\n",
    "#     predictions[test_indices] <- pred\n",
    "#     results[[fold]] <- calculate_metrics(pred, y_test)\n",
    "#   }\n",
    "  \n",
    "#   return(list(predictions = predictions, metrics = results))\n",
    "# }\n",
    "\n",
    "# # ---- 6. Enhanced Visualization Functions ----\n",
    "# prepare_metrics_data <- function(results_list) {\n",
    "#   plot_data <- data.frame()\n",
    "  \n",
    "#   for (model in names(results_list)) {\n",
    "#     metrics <- results_list[[model]]$metrics\n",
    "    \n",
    "#     # Extract metrics\n",
    "#     accuracies <- sapply(metrics, function(x) x$accuracy)\n",
    "#     sensitivities <- sapply(metrics, function(x) x$sensitivity)\n",
    "#     specificities <- sapply(metrics, function(x) x$specificity)\n",
    "    \n",
    "#     # Combine into data frame\n",
    "#     model_data <- data.frame(\n",
    "#       Model = rep(model, 3 * length(accuracies)),\n",
    "#       Metric = rep(c(\"Accuracy\", \"Sensitivity\", \"Specificity\"), each = length(accuracies)),\n",
    "#       Value = c(accuracies, sensitivities, specificities)\n",
    "#     )\n",
    "    \n",
    "#     plot_data <- rbind(plot_data, model_data)\n",
    "#   }\n",
    "  \n",
    "#   # Convert Model to factor with specific order\n",
    "#   plot_data$Model <- factor(plot_data$Model, \n",
    "#                            levels = c(\"logistic\", \"lda\", \"qda\", \"naive_bayes\", \"tree\", \"rf\"),\n",
    "#                            labels = c(\"Logistic\", \"LDA\", \"QDA\", \"Naive Bayes\", \"Decision Tree\", \"Random Forest\"))\n",
    "  \n",
    "#   return(plot_data)\n",
    "# }\n",
    "\n",
    "# create_metric_plot <- function(data, metric_name) {\n",
    "#   metric_data <- data[data$Metric == metric_name, ]\n",
    "  \n",
    "#   ggplot(metric_data, aes(x = Model, y = Value, fill = Model)) +\n",
    "#     geom_boxplot(alpha = 0.8) +\n",
    "#     scale_fill_brewer(palette = \"Set2\") +\n",
    "#     theme_minimal() +\n",
    "#     labs(title = paste(metric_name, \"by Model\"),\n",
    "#          y = metric_name,\n",
    "#          x = \"\") +\n",
    "#     theme(\n",
    "#       plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n",
    "#       axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "#       legend.position = \"none\",\n",
    "#       panel.grid.major.x = element_blank(),\n",
    "#       panel.border = element_rect(fill = NA, color = \"gray80\"),\n",
    "#       plot.margin = margin(5, 10, 5, 10)\n",
    "#     ) +\n",
    "#     scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1))\n",
    "# }\n",
    "\n",
    "# plot_classification_results <- function(results_list, title = \"Model Performance Comparison\") {\n",
    "#   # Prepare data\n",
    "#   plot_data <- prepare_metrics_data(results_list)\n",
    "  \n",
    "#   # Create individual plots\n",
    "#   accuracy_plot <- create_metric_plot(plot_data, \"Accuracy\")\n",
    "#   sensitivity_plot <- create_metric_plot(plot_data, \"Sensitivity\")\n",
    "#   specificity_plot <- create_metric_plot(plot_data, \"Specificity\")\n",
    "  \n",
    "#   # Combine plots\n",
    "#   combined_plot <- grid.arrange(\n",
    "#     accuracy_plot, sensitivity_plot, specificity_plot,\n",
    "#     ncol = 1,\n",
    "#     top = grid::textGrob(title, gp = grid::gpar(fontsize = 14, fontface = \"bold\"))\n",
    "#   )\n",
    "  \n",
    "#   return(combined_plot)\n",
    "# }\n",
    "\n",
    "# create_summary_table <- function(results_list) {\n",
    "#   summary_data <- data.frame()\n",
    "  \n",
    "#   for (model in names(results_list)) {\n",
    "#     metrics <- results_list[[model]]$metrics\n",
    "    \n",
    "#     # Calculate summary statistics\n",
    "#     model_summary <- data.frame(\n",
    "#       Model = model,\n",
    "#       Metric = c(\"Accuracy\", \"Sensitivity\", \"Specificity\"),\n",
    "#       Mean = c(\n",
    "#         mean(sapply(metrics, function(x) x$accuracy)),\n",
    "#         mean(sapply(metrics, function(x) x$sensitivity)),\n",
    "#         mean(sapply(metrics, function(x) x$specificity))\n",
    "#       ),\n",
    "#       SD = c(\n",
    "#         sd(sapply(metrics, function(x) x$accuracy)),\n",
    "#         sd(sapply(metrics, function(x) x$sensitivity)),\n",
    "#         sd(sapply(metrics, function(x) x$specificity))\n",
    "#       )\n",
    "#     )\n",
    "    \n",
    "#     summary_data <- rbind(summary_data, model_summary)\n",
    "#   }\n",
    "  \n",
    "#   # Create summary table plot\n",
    "#   summary_plot <- ggplot(summary_data, aes(x = Model, y = Metric)) +\n",
    "#     geom_tile(aes(fill = Mean), color = \"white\") +\n",
    "#     geom_text(aes(label = sprintf(\"%.3f\\n(±%.3f)\", Mean, SD)), size = 3) +\n",
    "#     scale_fill_gradient2(low = \"white\", high = \"#4CAF50\", midpoint = 0.5) +\n",
    "#     theme_minimal() +\n",
    "#     labs(title = \"Summary Statistics\",\n",
    "#          fill = \"Mean Value\") +\n",
    "#     theme(\n",
    "#       axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "#       plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5)\n",
    "#     )\n",
    "  \n",
    "#   return(summary_plot)\n",
    "# }\n",
    "\n",
    "# # ---- 7. Statistical Comparison Function ----\n",
    "# perform_mcnemar_tests <- function(results_list) {\n",
    "#   models <- names(results_list)\n",
    "#   n_models <- length(models)\n",
    "#   mcnemar_results <- matrix(NA, n_models, n_models)\n",
    "#   rownames(mcnemar_results) <- models\n",
    "#   colnames(mcnemar_results) <- models\n",
    "  \n",
    "#   for (i in 1:(n_models-1)) {\n",
    "#     for (j in (i+1):n_models) {\n",
    "#       pred_i <- results_list[[models[i]]]$predictions\n",
    "#       pred_j <- results_list[[models[j]]]$predictions\n",
    "      \n",
    "#       # Create contingency table\n",
    "#       table_ij <- table(pred_i == y, pred_j == y)\n",
    "      \n",
    "#       # Perform McNemar's test\n",
    "#       test_result <- mcnemar.test(table_ij)\n",
    "#       mcnemar_results[i,j] <- test_result$p.value\n",
    "#       mcnemar_results[j,i] <- test_result$p.value\n",
    "#     }\n",
    "#   }\n",
    "  \n",
    "#   diag(mcnemar_results) <- 1\n",
    "#   return(mcnemar_results)\n",
    "# }\n",
    "\n",
    "# # ---- 8. Run Complete Analysis ----\n",
    "# # Define models to test\n",
    "# model_types <- c(\"logistic\", \"lda\", \"qda\", \"naive_bayes\", \"tree\", \"rf\")\n",
    "\n",
    "# # Run analysis without preprocessing\n",
    "# cat(\"\\nRunning analysis without preprocessing...\\n\")\n",
    "# results_original <- list()\n",
    "# for (model in model_types) {\n",
    "#   cat(sprintf(\"Training %s model...\\n\", model))\n",
    "#   results_original[[model]] <- nested_cv(X, y, model_type = model, \n",
    "#                                        preprocess_params = list(scale = FALSE, \n",
    "#                                                              remove_correlated = FALSE,\n",
    "#                                                              handle_outliers = FALSE))\n",
    "# }\n",
    "\n",
    "# # Run analysis with preprocessing\n",
    "# cat(\"\\nRunning analysis with preprocessing...\\n\")\n",
    "# results_preprocessed <- list()\n",
    "# for (model in model_types) {\n",
    "#   cat(sprintf(\"Training %s model...\\n\", model))\n",
    "#   results_preprocessed[[model]] <- nested_cv(X, y, model_type = model, \n",
    "#                                            preprocess_params = list(scale = TRUE, \n",
    "#                                                                  remove_correlated = TRUE,\n",
    "#                                                                  handle_outliers = TRUE))\n",
    "# }\n",
    "\n",
    "# # ---- 9. Generate and Save Results ----\n",
    "# # Create output directory if it doesn't exist\n",
    "# dir.create(\"analysis_results\", showWarnings = FALSE)\n",
    "\n",
    "# # Save plots to PDF\n",
    "# pdf(\"analysis_results/classification_results.pdf\", height = 12, width = 10)\n",
    "\n",
    "# # Plot results without preprocessing\n",
    "# plots_original <- plot_classification_results(results_original, \n",
    "#     \"Model Performance Comparison (Without Preprocessing)\")\n",
    "# summary_original <- create_summary_table(results_original)\n",
    "# grid.arrange(plots_original, summary_original, ncol = 1, heights = c(3, 1))\n",
    "\n",
    "# # Plot results with preprocessing\n",
    "# plots_preprocessed <- plot_classification_results(results_preprocessed, \n",
    "#     \"Model Performance Comparison (With Preprocessing)\")\n",
    "# summary_preprocessed <- create_summary_table(results_preprocessed)\n",
    "# grid.arrange(plots_preprocessed, summary_preprocessed, ncol = 1, heights = c(3, 1))\n",
    "\n",
    "# dev.off()\n",
    "\n",
    "# # Perform statistical comparisons\n",
    "# mcnemar_results_original <- perform_mcnemar_tests(results_original)\n",
    "# mcnemar_results_preprocessed <- perform_mcnemar_tests(results_preprocessed)\n",
    "\n",
    "# # Save statistical results\n",
    "# sink(\"analysis_results/statistical_results.txt\")\n",
    "\n",
    "# cat(\"\\nMcNemar's Test Results (Original Data):\\n\")\n",
    "# print(round(mcnemar_results_original, 4))\n",
    "\n",
    "# cat(\"\\nMcNemar's Test Results (Preprocessed Data):\\n\")\n",
    "# print(round(mcnemar_results_preprocessed, 4))\n",
    "\n",
    "# # Print detailed metrics\n",
    "# print_model_metrics <- function(results_list, title) {\n",
    "#   cat(\"\\n\", title, \"\\n\", sep=\"\")\n",
    "#   cat(paste(rep(\"-\", nchar(title)), collapse=\"\"), \"\\n\")\n",
    "  \n",
    "#   for (model in names(results_list)) {\n",
    "#     metrics <- results_list[[model]]$metrics\n",
    "#     avg_accuracy <- mean(sapply(metrics, function(x) x$accuracy))\n",
    "#     avg_sensitivity <- mean(sapply(metrics, function(x) x$sensitivity))\n",
    "#     avg_specificity <- mean(sapply(metrics, function(x) x$specificity))\n",
    "    \n",
    "#     sd_accuracy <- sd(sapply(metrics, function(x) x$accuracy))\n",
    "#     sd_sensitivity <- sd(sapply(metrics, function(x) x$sensitivity))\n",
    "#     sd_specificity <- sd(sapply(metrics, function(x) x$specificity))\n",
    "    \n",
    "#     cat(sprintf(\"\\n%s:\\n\", model))\n",
    "#     cat(sprintf(\"Accuracy: %.4f (±%.4f)\\n\", avg_accuracy, sd_accuracy))\n",
    "#     cat(sprintf(\"Sensitivity: %.4f (±%.4f)\\n\", avg_sensitivity, sd_sensitivity))\n",
    "#     cat(sprintf(\"Specificity: %.4f (±%.4f)\\n\", avg_specificity, sd_specificity))\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# # Print detailed metrics for both analyses\n",
    "# cat(\"\\nDetailed Model Metrics:\\n\")\n",
    "# cat(\"=====================\\n\")\n",
    "# print_model_metrics(results_original, \"Results without Preprocessing\")\n",
    "# print_model_metrics(results_preprocessed, \"Results with Preprocessing\")\n",
    "\n",
    "# # Close the output file\n",
    "# sink()\n",
    "\n",
    "# # Create comparison plots of preprocessed vs non-preprocessed results\n",
    "# pdf(\"analysis_results/preprocessing_comparison.pdf\", height = 8, width = 12)\n",
    "\n",
    "# # Prepare combined data for comparison\n",
    "# prepare_comparison_data <- function(original_results, preprocessed_results) {\n",
    "#   original_data <- prepare_metrics_data(original_results)\n",
    "#   original_data$Preprocessing <- \"Without Preprocessing\"\n",
    "  \n",
    "#   preprocessed_data <- prepare_metrics_data(preprocessed_results)\n",
    "#   preprocessed_data$Preprocessing <- \"With Preprocessing\"\n",
    "  \n",
    "#   rbind(original_data, preprocessed_data)\n",
    "# }\n",
    "\n",
    "# comparison_data <- prepare_comparison_data(results_original, results_preprocessed)\n",
    "\n",
    "# # Create comparison plots\n",
    "# for (metric in c(\"Accuracy\", \"Sensitivity\", \"Specificity\")) {\n",
    "#   metric_data <- comparison_data[comparison_data$Metric == metric, ]\n",
    "  \n",
    "#   p <- ggplot(metric_data, aes(x = Model, y = Value, fill = Preprocessing)) +\n",
    "#     geom_boxplot(position = position_dodge(width = 0.8), alpha = 0.8) +\n",
    "#     scale_fill_brewer(palette = \"Set2\") +\n",
    "#     theme_minimal() +\n",
    "#     labs(title = paste(metric, \"Comparison: Preprocessing Effect\"),\n",
    "#          y = metric,\n",
    "#          x = \"\") +\n",
    "#     theme(\n",
    "#       plot.title = element_text(size = 12, face = \"bold\", hjust = 0.5),\n",
    "#       axis.text.x = element_text(angle = 45, hjust = 1),\n",
    "#       legend.position = \"top\",\n",
    "#       panel.grid.major.x = element_blank(),\n",
    "#       panel.border = element_rect(fill = NA, color = \"gray80\"),\n",
    "#       plot.margin = margin(5, 10, 5, 10)\n",
    "#     ) +\n",
    "#     scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1))\n",
    "  \n",
    "#   print(p)\n",
    "# }\n",
    "\n",
    "# dev.off()\n",
    "\n",
    "# # Print completion message\n",
    "# cat(\"\\nAnalysis complete! Results have been saved to the 'analysis_results' directory.\\n\")\n",
    "# cat(\"Generated files:\\n\")\n",
    "# cat(\"1. classification_results.pdf - Contains performance plots and summary tables\\n\")\n",
    "# cat(\"2. preprocessing_comparison.pdf - Shows the effect of preprocessing\\n\")\n",
    "# cat(\"3. statistical_results.txt - Contains detailed metrics and statistical tests\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
